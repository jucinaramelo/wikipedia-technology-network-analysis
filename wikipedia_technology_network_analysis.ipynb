{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Desinstala o pacote python-louvain (e seu nome de importação 'community')\n",
        "!pip uninstall python-louvain -y\n",
        "# Tenta desinstalar o módulo 'community' que pode estar causando o conflito\n",
        "!pip uninstall community -y\n",
        "# Remove o cache de importação\n",
        "import sys\n",
        "if 'community' in sys.modules:\n",
        "    del sys.modules['community']"
      ],
      "metadata": {
        "id": "QB2b3it_FJmK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala a dependência Louvain com o gerenciador de sistema\n",
        "!apt-get install liblapack-dev\n",
        "!pip install python-louvain"
      ],
      "metadata": {
        "id": "hvKwzDSGFZmd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia requests beautifulsoup4 networkx nxviz python-louvain pandas matplotlib seaborn tqdm lxml\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b5Uj4-9O_dGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importações e configurações\n",
        "import time\n",
        "import wikipedia\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from operator import itemgetter\n",
        "# import community as community_louvain\n"
      ],
      "metadata": {
        "id": "pHEaLsNR_T9V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB979OBF9_0o"
      },
      "outputs": [],
      "source": [
        "# Trabalho Final\n",
        "\n",
        "# parâmetros gerais\n",
        "SEEDS = [\n",
        "    \"Python (programming language)\",\n",
        "    \"Football\",\n",
        "    \"Global warmin\",\n",
        "    \"Energy crisis\",\n",
        "    \"Eclipse\"\n",
        "]\n",
        "MAX_DEPTH = 2            # altura < 3\n",
        "MAX_LINKS_PER_PAGE = 100  # heurística para evitar explosão\n",
        "REQUEST_SLEEP = 0.4\n",
        "STOPS = (\n",
        "    \"International Standard Serial Number\",\n",
        "    \"International Standard Book Number\",\n",
        "    \"National Diet Library\",\n",
        "    \"International Standard Name Identifier\",\n",
        "    \"International Standard Book Number (Identifier)\",\n",
        "    \"Pubmed Identifier\",\n",
        "    \"Pubmed Central\",\n",
        "    \"Digital Object Identifier\",\n",
        "    \"Arxiv\",\n",
        "    \"Proc Natl Acad Sci Usa\",\n",
        "    \"Bibcode\",\n",
        "    \"Library Of Congress Control Number\",\n",
        "    \"Jstor\",\n",
        "    \"Doi (Identifier)\",\n",
        "    \"Isbn (Identifier)\",\n",
        "    \"Pmid (Identifier)\",\n",
        "    \"Arxiv (Identifier)\",\n",
        "    \"Bibcode (Identifier)\"\n",
        ")\n",
        "\n",
        "# %%\n",
        "# 2. Funções de coleta\n",
        "\n",
        "def crawl_seed(seed, max_depth=MAX_DEPTH, max_links_per_page=MAX_LINKS_PER_PAGE, sleep=REQUEST_SLEEP):\n",
        "    \"\"\"Roda BFS até max_depth (0=seed) para a seed fornecida.\n",
        "    Retorna um DiGraph (edges: page -> linked_page).\n",
        "    \"\"\"\n",
        "    todo_lst = [(0, seed.title())]\n",
        "    todo_set = {seed.title()}\n",
        "    done_set = set()\n",
        "    g = nx.DiGraph()\n",
        "\n",
        "    while todo_lst:\n",
        "        layer, page = todo_lst.pop(0)\n",
        "        if layer > max_depth - 1:\n",
        "            break\n",
        "        if page in done_set:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            wiki = wikipedia.page(page)\n",
        "        except Exception as e:\n",
        "            # falha ao carregar => pular\n",
        "            done_set.add(page)\n",
        "            continue\n",
        "\n",
        "        done_set.add(page)\n",
        "        count = 0\n",
        "        for link in wiki.links:\n",
        "            link = link.title()\n",
        "            if (link not in STOPS) and (not link.startswith(\"List Of\")) and (\":\" not in link):\n",
        "                g.add_edge(page, link)\n",
        "                if (link not in todo_set) and (link not in done_set) and (layer + 1 <= max_depth - 1):\n",
        "                    todo_lst.append((layer + 1, link))\n",
        "                    todo_set.add(link)\n",
        "                count += 1\n",
        "                if count >= max_links_per_page:\n",
        "                    break\n",
        "        time.sleep(sleep)\n",
        "\n",
        "    return g\n",
        "\n",
        "# %%\n",
        "# 3. Coletar para todas as seeds e mesclar\n",
        "start_time_coleta = time.time()\n",
        "\n",
        "graphs = []\n",
        "for seed in SEEDS:\n",
        "    print(f\"Coletando seed: {seed}\")\n",
        "    g = crawl_seed(seed)\n",
        "    print(f\"Seed {seed}: nós={len(g.nodes())}, arestas={g.number_of_edges()}\")\n",
        "    graphs.append(g)\n",
        "\n",
        "G = nx.DiGraph()\n",
        "for g in graphs:\n",
        "    G.add_nodes_from(g.nodes())\n",
        "    G.add_edges_from(g.edges())\n",
        "\n",
        "end_time_coleta = time.time()\n",
        "coleta_duration = end_time_coleta - start_time_coleta\n",
        "\n",
        "print(f\"Grafo mesclado: nós={len(G.nodes())}, arestas={G.number_of_edges()}\")\n",
        "\n",
        "\n",
        "# 4. Limpeza\n",
        "G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "# Identifica e contrai duplicatas simples ('network' e 'networks')\n",
        "duplicates = [(node, node+\"s\") for node in list(G) if node+\"s\" in G]\n",
        "for dup in duplicates:\n",
        "    # nx.contracted_nodes cria o atributo 'contraction' no nó, que o GraphML não suporta.\n",
        "    G = nx.contracted_nodes(G, *dup, self_loops=False)\n",
        "\n",
        "# Identifica e contrai duplicatas com hífen\n",
        "duplicates = [(x, y) for x, y in [(node, node.replace(\"-\", \" \")) for node in list(G)] if x != y and y in G]\n",
        "for dup in duplicates:\n",
        "    G = nx.contracted_nodes(G, *dup, self_loops=False)\n",
        "\n",
        "# Remove o atributo 'contraction' de todos os nós que foram mesclados.\n",
        "nodes_to_clean = list(G.nodes())\n",
        "for node in nodes_to_clean:\n",
        "    if 'contraction' in G.nodes[node]:\n",
        "        del G.nodes[node]['contraction']\n",
        "\n",
        "print(f\"Após limpeza: nós={len(G.nodes())}, arestas={G.number_of_edges()}\")\n",
        "\n",
        "# %%\n",
        "start_time_metricas = time.time()\n",
        "# 6. Cálculo de métricas\n",
        "G_und = nx.Graph()\n",
        "G_und.add_nodes_from(G.nodes(data=True))\n",
        "G_und.add_edges_from(G.edges())\n",
        "\n",
        "print(\"Calculando métricas... isso pode demorar dependendo do tamanho do grafo\")\n",
        "\n",
        "# degree (int)\n",
        "deg_dict = dict(G_und.degree())\n",
        "nx.set_node_attributes(G_und, deg_dict, \"degree\")\n",
        "\n",
        "# centralidades\n",
        "deg_c = nx.degree_centrality(G_und)\n",
        "clos = nx.closeness_centrality(G_und)\n",
        "betw = nx.betweenness_centrality(G_und, normalized=True)\n",
        "try:\n",
        "    eig = nx.eigenvector_centrality(G_und, max_iter=200)\n",
        "except Exception as e:\n",
        "    print(\"Eigenvector não convergiu:\", e)\n",
        "    eig = {n: 0.0 for n in G_und.nodes()}\n",
        "\n",
        "nx.set_node_attributes(G_und, deg_c, \"degree_centrality\")\n",
        "nx.set_node_attributes(G_und, clos, \"closeness\")\n",
        "nx.set_node_attributes(G_und, betw, \"betweenness\")\n",
        "nx.set_node_attributes(G_und, eig, \"eigenvector\")\n",
        "\n",
        "# core_number\n",
        "core_n = nx.core_number(G_und)\n",
        "nx.set_node_attributes(G_und, core_n, \"core_number\")\n",
        "\n",
        "# comunidades (Louvain)\n",
        "import community\n",
        "partition = community.best_partition(G_und)\n",
        "nx.set_node_attributes(G_und, partition, \"community\")\n",
        "\n",
        "# exportar métricas\n",
        "rows = []\n",
        "for n in G_und.nodes():\n",
        "    rows.append({\n",
        "        \"node\": n,\n",
        "        \"degree\": G_und.nodes[n].get(\"degree\", 0),\n",
        "        \"degree_centrality\": G_und.nodes[n].get(\"degree_centrality\", 0),\n",
        "        \"closeness\": G_und.nodes[n].get(\"closeness\", 0),\n",
        "        \"betweenness\": G_und.nodes[n].get(\"betweenness\", 0),\n",
        "        \"eigenvector\": G_und.nodes[n].get(\"eigenvector\", 0),\n",
        "        \"core_number\": G_und.nodes[n].get(\"core_number\", 0),\n",
        "        \"community\": G_und.nodes[n].get(\"community\", -1)\n",
        "    })\n",
        "\n",
        "df_metrics = pd.DataFrame(rows)\n",
        "df_metrics.to_csv(\"metrics_wiki_tech.csv\", index=False)\n",
        "\n",
        "end_time_metricas = time.time()\n",
        "metricas_duration = end_time_metricas - start_time_metricas\n",
        "print(\"CSV de métricas salvo: metrics_wiki_tech.csv\")\n",
        "\n",
        "# %%\n",
        "# 7. Export para Gephi (GEXF) — inclui atributos de nó\n",
        "nx.write_gexf(G_und, \"wiki_tech_merged.gexf\")\n",
        "print(\"GEXF salvo: wiki_tech_merged.gexf\")\n",
        "\n",
        "# %%\n",
        "# 8. Plots básicos\n",
        "import matplotlib as mpl\n",
        "\n",
        "pos = nx.spring_layout(G_und, seed=42, k=0.15)\n",
        "DEGREE_THRESHOLD = 40\n",
        "\n",
        "def plot_metric(metric, fname=None):\n",
        "    fig, ax = plt.subplots(1,1,figsize=(12,10)) # Tamanho da figura\n",
        "    vals = [G_und.nodes[n].get(metric, 0) for n in G_und.nodes()]\n",
        "\n",
        "    # 1. CRIA O DICIONÁRIO DE RÓTULOS FILTRADO\n",
        "    labels_to_show = {\n",
        "        node: node\n",
        "        for node in G_und.nodes()\n",
        "        if G_und.nodes[node].get(\"degree\", 0) >= DEGREE_THRESHOLD # Só inclui nós acima do limite\n",
        "    }\n",
        "\n",
        "    nx.draw_networkx_edges(G_und, pos=pos, alpha=0.2, ax=ax)\n",
        "\n",
        "    # Desenha os nós (o tamanho já é proporcional ao degree)\n",
        "    nx.draw_networkx_nodes(G_und,\n",
        "                           pos=pos,\n",
        "                           node_color=vals,\n",
        "                           cmap=plt.cm.jet,\n",
        "                           node_size=[max(10, G_und.nodes[n].get(\"degree\",1)*15) for n in G_und.nodes()], # Aumentei o multiplicador (15) para destacar mais\n",
        "                           ax=ax)\n",
        "\n",
        "    # 2. DESENHA SOMENTE OS RÓTULOS FILTRADOS\n",
        "    nx.draw_networkx_labels(G_und,\n",
        "                            pos=pos,\n",
        "                            labels=labels_to_show, # <-- Usa o dicionário filtrado\n",
        "                            font_size=8,\n",
        "                            font_color='black', # Use preto se o fundo do nó for vermelho/amarelo\n",
        "                            ax=ax)\n",
        "\n",
        "    # ... (Restante do código da barra de cores)\n",
        "    sm = mpl.cm.ScalarMappable(cmap=plt.cm.jet, norm=mpl.colors.Normalize(vmin=min(vals), vmax=max(vals)))\n",
        "    sm.set_array([])\n",
        "    cbar = fig.colorbar(sm, ax=ax)\n",
        "    cbar.set_label(metric, rotation=270, labelpad=15)\n",
        "    plt.axis('off')\n",
        "    if fname:\n",
        "        plt.savefig(fname, dpi=300, bbox_inches='tight', transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "# Chame as funções de plotagem\n",
        "plot_metric('degree', fname='degree_wiki.png')\n",
        "plot_metric('betweenness', fname='betweenness_wiki.png')\n",
        "\n",
        "\n",
        "# Fim do notebook\n",
        "print('Notebook pronto. Revise parâmetros (MAX_LINKS_PER_PAGE, MAX_DEPTH, SEEDS) antes de rodar para ajustar tamanho do grafo.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Grafo mesclado: nós={len(G.nodes())}, arestas={G.number_of_edges()}\")\n",
        "print(f\"Tempo total de coleta: {coleta_duration:.2f} segundos\")\n",
        "print(f\"Tempo total de calculo de metricas: {metricas_duration:.2f} segundos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRhDev1rsCkd",
        "outputId": "a63cec58-8a34-4068-daff-0829067e4df5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grafo mesclado: nós=18184, arestas=41477\n",
            "Tempo total de coleta: 327.39 segundos\n",
            "Tempo total de calculo de metricas: 2371.49 segundos\n"
          ]
        }
      ]
    }
  ]
}